# AlphaApple [사과게임 (fruitbox) 해결 AI]

## 전체 아키텍처
1. 환경(Environment): 10×17 격자(실제 게임 규격에 맞춰 조정)에서 합이 10인 직사각형을 선택하면 사과가 사라지고 점수는 “사과 개수”.
2. 행동(Action) 표현: (r1,c1,r2,c2) 직사각형을 정해진 인덱스로 매핑한 “고정 크기 이산 행동공간”. 유효하지 않은 행동은 마스크.
3. 정책/가치 신경망: CNN(or 작은 Transformer)로 격자를 인코딩 → Masked-PPO(sb3-contrib)로 학습.
4. 커리큘럼: 작은 보드 → 점차 10×17로 확장.
5. 평가: 평균 점수, 풀클리어 비율, 평균 수(手) 수.
6. 배포: PyTorch → ONNX 변환 → (선택) 정밀도 낮춰 동적/정적 양자화 → Hugging Face 업로드.
7. Chrome 확장: content script가 도메인 DOM에서 그리드 추출 → onnxruntime-web으로 모델 추론 → 마우스 드래그 이벤트로 실행. (또는 HF CDN에서 모델 받아오기.)

## 문제 정의
- 상태(Observation)
  - $R\times C$ 보드(예: R=17, C=10). 각 칸은 1~9.
  - 텐서 형태: $[1, R, C]$ int8 혹은 원-핫 $[9, R, C]$.

- 행동(Action)
  - 모든 직사각형은 (r1≤r2, c1≤c2)로 정의.
  - 행동 개수 $N = \frac{R(R+1)}{2}\cdot \frac{C(C+1)}{2}$. (예: 17×10이면 153×55=8,415개. 충분히 감당 가능.)
  - 마스킹: 합이 10이 아닌 직사각형은 선택 불가로 mask=-∞.

- 보상(Reward)
  - 직사각형 합=10이면 지운 사과 개수만큼 +r (직사각형 면적).
  - 스텝 패널티 -0.01, 풀클리어 보너스 +20 등으로 속도/완결성 유도.

- 에피소드 종료
  - 유효 행동 없음 또는 스텝 상한(예: 100~200).

## RL 학습 설계
- 알고리즘
  - Masked-PPO (sb3-contrib): 대형 이산 행동공간 + 행동마스크에 적합.
  - 대안: Q러닝(dueling DQN) + 후보 임베딩, 혹은 두-코너 자동회귀(policy가 두 점을 순차적으로 샘플) 도 가능. 처음엔 Masked-PPO가 가장 빠름.

- 네트워크
  - 간단 CNN(3×3 conv 3~5층) → 전역 평균 풀링 → policy logits(N), value.
  - 또는 작은 Transformer(patchify 2×2 or 1×1)로 보드 컨텍스트를 학습.

- 커리큘럼 & 탐색 보강
  - 보드 크기 (6×7 → 8×12 → 10×17) 순서로 확대.
  - Imitation Warm-start: 휴리스틱/탐색 솔버가 고른 Top-k 직사각형을 행동 레이블로 하여 행동 클로닝으로 초기화 → PPO 미세조정.

- 데이터 생성
  - 균등(1~9)로 보드 생성 후 $\sum \bmod 10$이 0이 되도록 조정(무작정 solvable을 강제할 필요는 없음).
  - (선택) 작은 보드에 대해 최적해 솔버(브루트/DP)로 optimal 행동값을 구해 imitation.